{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from books import get_book_dataframe, get_book_features\n",
    "data_path = '../data/goodbooks-10k/'\n",
    "book_features = get_book_features(get_book_dataframe(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/doruk/Desktop/Playground/book-recommender-system/Evaluation/util.py:48: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  row = ratings.user_id.astype('category', categories=users).cat.codes\n",
      "/mnt/c/Users/doruk/Desktop/Playground/book-recommender-system/Evaluation/util.py:49: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  col = ratings.book_id.astype('category', categories=books).cat.codes\n"
     ]
    }
   ],
   "source": [
    "from joiner import get_ratings, get_reduced_joint\n",
    "goodreads_path = '../data/goodbooks-10k/ratings.csv'\n",
    "amazon_path = '../data/amazon/ratings_amazon.csv'\n",
    "spr = get_ratings(goodreads_path, amazon_path, min_amazon_items = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_components = 300\n",
    "ratings_components = 300\n",
    "learning_rate = 0.01\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_sparse, reduce_matrix\n",
    "reduced_item_features, _, _ = reduce_matrix(book_features, n_components = features_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_validation import ColumnwiseKFold\n",
    "n_folds = 5\n",
    "kf = ColumnwiseKFold(n_folds, random_seed = 30)\n",
    "a = [a for a in enumerate(kf.split(spr))]\n",
    "a = a[0]\n",
    "i, (X, (user_incides, item_indices)) = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, rating_VT = reduce_matrix(X, n_components = ratings_components)\n",
    "reduced_item_ratings = rating_VT.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = get_reduced_joint(reduced_item_ratings, reduced_item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_items = tf.placeholder(tf.float32, (None, items.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1 = tf.layers.dense(tf_items, 200, activation = tf.nn.relu)\n",
    "enc2 = tf.layers.dense(enc1, 150, activation = tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec1 = tf.layers.dense(enc1, 200, activation = tf.nn.relu)\n",
    "dec2 = tf.layers.dense(enc1, items.shape[1], activation = tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = tf.reduce_mean(tf.pow(tf_items - dec2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "n_train = items.shape[0]\n",
    "n_batches = ceil(n_train * 1.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_items = [items[i * batch_size: min((i + 1) * batch_size, n_train)] for i in range(n_batches)]\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005409082\n",
      "0.0035899584\n",
      "0.0023452835\n",
      "0.0018511331\n",
      "0.0019377748\n",
      "0.0017509753\n",
      "0.0018621528\n",
      "0.0017680447\n",
      "0.0016930173\n",
      "0.0016889831\n",
      "0.001675233\n",
      "0.0015903157\n",
      "0.0016270886\n",
      "0.0016930905\n",
      "0.0015557241\n",
      "0.0016346042\n",
      "0.001701319\n",
      "0.0016300523\n",
      "0.0016291173\n",
      "0.0015790871\n",
      "0.0015808585\n",
      "0.0014857096\n",
      "0.0015043818\n",
      "0.0015300049\n",
      "0.0016117733\n",
      "0.0015859832\n",
      "0.0014950776\n",
      "0.0015603435\n",
      "0.00151159\n",
      "0.0015609588\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "for i in range(3000):\n",
    "    _, l = sess.run([optimizer, reconstruction_loss],\n",
    "                    feed_dict={\n",
    "                        tf_items: x_items[i % len(x_items)]})\n",
    "    if i % 100 == 0:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "[reprs] = sess.run([enc2], feed_dict = {tf_items: items})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 150)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90049046, 0.70004326], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\" Returns the root mean squared error \"\"\"\n",
    "    return np.sqrt(np.mean(np.square(y_true - y_pred)))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    \"\"\" Returns the mean absolute error \"\"\"\n",
    "    return np.mean(np.absolute(y_true - y_pred))\n",
    "\n",
    "def evaluate(X_orig, X_held_out, sim, user_indices, item_indices):\n",
    "    held_outs = X_orig[(user_indices, item_indices)]\n",
    "    predictions = []\n",
    "    for u, i in zip(user_indices, item_indices):\n",
    "        user_arr = np.asarray(X_held_out[u,:].todense())[0]\n",
    "        norm_const = np.sum((user_arr != 0) * (sim[i,:]))\n",
    "        pred_rating = np.sum(user_arr * sim[i,:])/norm_const\n",
    "        predictions.append(pred_rating)\n",
    "    rmse_ = rmse(held_outs, np.array(predictions))\n",
    "    mae_ = mae(held_outs, np.array(predictions))\n",
    "    return np.array([rmse_, mae_])\n",
    "\n",
    "sim = (cosine_similarity(reprs) + 1) / 2\n",
    "evaluate(spr, X, sim, user_incides, item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
